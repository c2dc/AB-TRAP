{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455503, 42) (380438, 41)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../1. [A]ttack dataset/2. Internet/attack_dataset.csv.gz\") # attack dataset\n",
    "bonafide = pd.read_csv('../../2. [B]onafide dataset/data/bonafide_dataset_20191121.csv.gz') # bonafide traffic from mawilab\n",
    "bonafide = pd.concat([bonafide, pd.read_csv('../../2. [B]onafide dataset/data/bonafide_dataset_20201110.csv.gz')])\n",
    "bonafide = pd.concat([bonafide, pd.read_csv('../../2. [B]onafide dataset/data/bonafide_dataset_20201129.csv.gz')])\n",
    "print(df.shape, bonafide.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonafide['label'] = \"bonafide\" # label column in the bonafide dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert specific hex fields to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['eth.type', 'ip.id', 'ip.flags', 'ip.checksum', 'ip.dsfield', 'tcp.flags', 'tcp.checksum']\n",
    "\n",
    "for field in fields:\n",
    "    df[field] = df[field].apply(lambda x: int(str(x), 16))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA with 0 and convert hex to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonafide = bonafide.fillna(0)\n",
    "for field in fields:\n",
    "    bonafide[field] = bonafide[field].apply(lambda x: int(str(x), 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with all packets (bonafide and attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([bonafide, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if there are packets with protocol field different than TCP (value 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and removed [52177] packets from the original dataset.\n"
     ]
    }
   ],
   "source": [
    "wrong_proto = full_data[full_data['ip.proto'] != 6]['label'].value_counts().values\n",
    "full_data = full_data[full_data['ip.proto'] == 6]\n",
    "print(\"Found and removed\", wrong_proto,\"packets from the original dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features not applicable to this work\n",
    "\n",
    "> Remove features from layer 2 (link):\n",
    "> - frame_info.time\n",
    "> - frame_info.encap_type\n",
    "> - frame_info.time_epoch\n",
    "> - frame_info.number\n",
    "> - frame_info.len\n",
    "> - frame_info.cap_len\n",
    "> - eth.type\n",
    "\n",
    "> Remove redundant features or non-variant (constant)\n",
    "> - ip.version - we are considering only IPv4\n",
    "> - ip.proto - we are considering only TCP\n",
    "> - ip.src\n",
    "> - ip.dst\n",
    "> - ip.flags\n",
    "> - tcp.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.drop(columns=['frame_info.time', 'frame_info.encap_type', 'frame_info.time_epoch', 'frame_info.number', \n",
    "                        'frame_info.len', 'frame_info.cap_len', 'eth.type', 'ip.flags', 'ip.src', 'ip.dst',\n",
    "                        'ip.version', 'ip.proto', 'tcp.flags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns with variance zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.drop(columns=['ip.hdr_len', 'ip.tos', 'ip.flags.rb', \n",
    "                        'ip.flags.mf', 'ip.frag_offset'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace labels by 0 (bonafide) and 1 (attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    455503\n",
       "0    328261\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.label[full_data.label == \"bonafide\"] = 0 # replace \"normal\" labels to 0\n",
    "full_data.label[full_data.label != 0] = 1 # replace all scan labels to 1\n",
    "full_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove more columns\n",
    ">\n",
    "> - checksum and acknowlegde are random\n",
    "> - tcp.dstport will tend to learn the testbed (some tools were targeted to specific services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.drop(columns=[\"ip.checksum\", \"tcp.checksum\", \n",
    "                        \"tcp.ack\", \"tcp.dstport\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# preprocessor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Params from Grid Search (previous step)\n",
    "\n",
    "- MLP: {'hidden_layer_sizes': 10}\n",
    "- SVM: {}\n",
    "- KNN: {'n_neighbors': 1}\n",
    "- XGB: {}\n",
    "-  NB: {}\n",
    "-  LR: {}\n",
    "-  RF: {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 15}\n",
    "-  DT: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 15}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ip.id', 'ip.flags.df', 'ip.ttl', 'ip.len', 'ip.dsfield',\n",
       "       'tcp.srcport', 'tcp.seq', 'tcp.len', 'tcp.hdr_len',\n",
       "       'tcp.flags.fin', 'tcp.flags.syn', 'tcp.flags.reset',\n",
       "       'tcp.flags.push', 'tcp.flags.ack', 'tcp.flags.urg',\n",
       "       'tcp.flags.cwr', 'tcp.window_size', 'tcp.urgent_pointer',\n",
       "       'tcp.options.mss_val'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = full_data.fillna(0)\n",
    "X = full_data.drop(columns = [\"label\"])\n",
    "y = full_data.label\n",
    "X = X.astype(int) \n",
    "X.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data - Pre-processing for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = StandardScaler()\n",
    "prep.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(prep, open('Models/preprocessor.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with the parameters determined with grid search in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=10)\n",
    "svm = LinearSVC()\n",
    "knn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)\n",
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "nb = GaussianNB()\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\", criterion=\"entropy\", max_depth=10, n_estimators=15, n_jobs=-1)\n",
    "dt = DecisionTreeClassifier(class_weight=None, criterion=\"entropy\", max_depth=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the _full model_ (supervised learning) with all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:14:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X, y)\n",
    "svm.fit(X, y)\n",
    "knn.fit(X, y)\n",
    "xgb.fit(X, y)\n",
    "nb.fit(X, y)\n",
    "lr.fit(X, y)\n",
    "rf.fit(X, y)\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the models to be used by the IDS application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(mlp, open('Models/mlp.pkl', 'wb'))\n",
    "dump(svm, open('Models/svm.pkl', 'wb'))\n",
    "dump(knn, open('Models/knn.pkl', 'wb'))\n",
    "dump(xgb, open('Models/xgb.pkl', 'wb'))\n",
    "dump(nb, open('Models/nb.pkl', 'wb'))\n",
    "dump(lr, open('Models/lr.pkl', 'wb'))\n",
    "dump(rf, open('Models/rf.pkl', 'wb'))\n",
    "dump(dt, open('Models/dt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
